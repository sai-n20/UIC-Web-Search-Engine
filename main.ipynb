{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sai\n",
      "[nltk_data]     Nadkarni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_list = stopwords.words('english')\n",
    "ps = PorterStemmer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawledList = []\n",
    "pageTitles = []\n",
    "webData = []\n",
    "\n",
    "with open('pageList.txt', \"rb\") as out:\n",
    "    crawledList = pickle.load(out)\n",
    "out.close()\n",
    "\n",
    "with open('pageTitles.txt', \"rb\") as out:\n",
    "    pageTitles = pickle.load(out)\n",
    "out.close()\n",
    "\n",
    "\n",
    "with open('webData.txt', \"rb\") as out:\n",
    "    webData = pickle.load(out)\n",
    "out.close()\n",
    "\n",
    "with open('page_rank.pkl', 'rb') as out:\n",
    "    pageRank = pickle.load(out)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(data):\n",
    "    cleanData = []\n",
    "    for file in data:\n",
    "        temp = file.split()\n",
    "        temp = [ps.stem(word) for word in temp]\n",
    "        temp = [item for item in temp if item not in stop_list]\n",
    "        temp = [item for item in temp if len(item) > 2]\n",
    "        temp = (\" \").join(temp)\n",
    "        for i in punctuation:\n",
    "            temp = temp.replace(i, \"\")\n",
    "        cleanData.append(temp)\n",
    "    return cleanData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and pre-processing webpage data, this may take 15-20 seconds\n",
      "Webpage data pre processing complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning and pre-processing webpage data, this may take 20-30 seconds\")\n",
    "cleanWebData = cleaner(webData)\n",
    "print(\"Webpage data pre processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidfs = vectorizer.fit_transform(cleanWebData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryTokenize(query):\n",
    "    temp = query.lower()\n",
    "    temp = temp.split()\n",
    "    temp = [ps.stem(word) for word in temp]\n",
    "    temp = [item for item in temp if item not in stop_list]\n",
    "    temp = [item for item in temp if len(item) > 2]\n",
    "    temp = (\" \").join(temp)\n",
    "    for i in punctuation:\n",
    "        temp = temp.replace(i, \"\")\n",
    "    return temp.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlateTitleURL(length):\n",
    "    pageNums = []\n",
    "    for i in range(length):\n",
    "        pageNums.append([int(bestFirstList[i][1]), 0.4 * bestFirstList[i][0] + 0.6 * pageRank.get(crawledList[int(bestFirstList[i][1])])])\n",
    "    \n",
    "    pageNums = sorted(pageNums, key= lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for item in range(length):\n",
    "        print(item + 1, \"-\" + pageTitles[pageNums[item][0]] + \"\\nhttps://\" + crawledList[pageNums[item][0]] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a search query: computer science\n",
      "\n",
      "\n",
      "\u001b[2J\n",
      "1 -computing | UIC Today\n",
      "https://today.uic.edu/tag/computing\n",
      "\n",
      "2 -UIC aims to break through gender gap in computer science | UIC Today\n",
      "https://today.uic.edu/uic-aims-to-break-through-gender-gap-in-computer-science\n",
      "\n",
      "3 -computer science | UIC Today\n",
      "https://today.uic.edu/tag/computer-science/page/2\n",
      "\n",
      "4 -computer science | UIC Today\n",
      "https://today.uic.edu/tag/computer-science\n",
      "\n",
      "5 -computer science | UIC Today\n",
      "https://today.uic.edu/tag/computer-science/page/3\n",
      "\n",
      "6 -computer science | UIC Today\n",
      "https://today.uic.edu/tag/computer-science/page/4\n",
      "\n",
      "7 -UIC receives ongoing funding to boost diversity in computer science | UIC Today\n",
      "https://today.uic.edu/uic-receives-ongoing-funding-to-boost-diversity-in-computer-science\n",
      "\n",
      "8 - Computer Science | University of Illinois at Chicago \n",
      "https://cs.uic.edu\n",
      "\n",
      "9 -NSF awards UIC $1.5M for new data science institute | UIC Today\n",
      "https://today.uic.edu/nsf-awards-uic-1-5m-for-new-data-science-institute\n",
      "\n",
      "10 -UIC offers new data science degree | UIC Today\n",
      "https://today.uic.edu/uic-offers-new-data-science-degree\n",
      "\n",
      "\n",
      "Fetch more pages?y\n",
      "\u001b[2J\n",
      "1 -computing | UIC Today\n",
      "https://today.uic.edu/tag/computing\n",
      "\n",
      "2 -UIC aims to break through gender gap in computer science | UIC Today\n",
      "https://today.uic.edu/uic-aims-to-break-through-gender-gap-in-computer-science\n",
      "\n",
      "3 -computer science | UIC Today\n",
      "https://today.uic.edu/tag/computer-science/page/2\n",
      "\n",
      "4 -computer science | UIC Today\n",
      "https://today.uic.edu/tag/computer-science\n",
      "\n",
      "5 -computer science | UIC Today\n",
      "https://today.uic.edu/tag/computer-science/page/3\n",
      "\n",
      "6 -computer science | UIC Today\n",
      "https://today.uic.edu/tag/computer-science/page/4\n",
      "\n",
      "7 -UIC receives ongoing funding to boost diversity in computer science | UIC Today\n",
      "https://today.uic.edu/uic-receives-ongoing-funding-to-boost-diversity-in-computer-science\n",
      "\n",
      "8 - Computer Science | University of Illinois at Chicago \n",
      "https://cs.uic.edu\n",
      "\n",
      "9 -NSF awards UIC $1.5M for new data science institute | UIC Today\n",
      "https://today.uic.edu/nsf-awards-uic-1-5m-for-new-data-science-institute\n",
      "\n",
      "10 -UIC offers new data science degree | UIC Today\n",
      "https://today.uic.edu/uic-offers-new-data-science-degree\n",
      "\n",
      "11 -Use assistive technologies\n",
      "https://library.uic.edu/help/article/1957/use-assistive-technologies\n",
      "\n",
      "12 - Electrical and Computer Engineering | University of Illinois at Chicago \n",
      "https://ece.uic.edu\n",
      "\n",
      "13 -Board grants key approvals for UICâ€™s new health care, engineering buildings | UIC Today\n",
      "https://today.uic.edu/board-grants-key-approvals-for-uics-new-health-care-engineering-buildings\n",
      "\n",
      "14 -NSF funded report focuses on classroom technology | UIC Today\n",
      "https://today.uic.edu/nsf-funded-report-focuses-on-classroom-technology\n",
      "\n",
      "15 -$2.5M NSF grant backs UIC mathematics research, career training program | UIC Today\n",
      "https://today.uic.edu/2-5m-nsf-grant-backs-uic-mathematics-research-career-training-program\n",
      "\n",
      "16 -UIC CHANCE expands Digital Scholars program | UIC Today\n",
      "https://today.uic.edu/uic-chance-expands-digital-scholars-program\n",
      "\n",
      "17 -Inspiring grads: Discovering passion for computer science pays off | UIC Today\n",
      "https://today.uic.edu/inspiring-grads-discovering-passion-for-computer-science-pays-off\n",
      "\n",
      "18 -Senior challenges fellow computer science students to prepare for technical interviews | UIC Today\n",
      "https://today.uic.edu/senior-challenges-fellow-computer-science-students-to-prepare-for-technical-interviews\n",
      "\n",
      "19 -Box your data, send it into the cloud | UIC Today\n",
      "https://today.uic.edu/box-your-data-send-it-into-the-cloud\n",
      "\n",
      "20 -UIC Chancellor's Undergraduate Research Award | Getting Course Credit\n",
      "https://ure.uic.edu/for_students_credit.php\n",
      "\n",
      "\n",
      "Fetch more pages?n\n"
     ]
    }
   ],
   "source": [
    "query = str(input(\"Enter a search query: \"))\n",
    "print('\\n')\n",
    "queryTokens = queryTokenize(query)\n",
    "queryVector = vectorizer.transform([' '.join(queryTokens)])\n",
    "simScores = cosine_similarity(tfidfs, queryVector)\n",
    "\n",
    "bestFirstList = []\n",
    "for i in range(len(cleanWebData)):\n",
    "    bestFirstList.append((simScores[i][0], i))\n",
    "\n",
    "bestFirstList = sorted(bestFirstList, key= lambda x: x[0], reverse=True)\n",
    "\n",
    "searcher = True\n",
    "toFetch = 10\n",
    "\n",
    "while searcher or morePages.lower() in {'y', 'yes'}:\n",
    "    searcher = False\n",
    "    correlateTitleURL(toFetch)\n",
    "    morePages = str(input(\"\\nFetch more pages?\"))\n",
    "    toFetch += 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pageTokens = {}\n",
    "# invertedIndex = {}\n",
    "# key = 1\n",
    "\n",
    "# for item in cleanWebData:\n",
    "#     temp = item.split()\n",
    "#     pageTokens[key] = temp\n",
    "    \n",
    "#     for token in temp:\n",
    "#         frequency = invertedIndex.setdefault(token, {}).get(key, 0)\n",
    "#         invertedIndex.setdefault(token, {})[key] = frequency + 1\n",
    "#     key += 1\n",
    "\n",
    "# mostFreq = {}\n",
    "\n",
    "# for page in pageTokens:\n",
    "#     mostFreq[page] = Counter(pageTokens[page]).most_common(1)[0][1]\n",
    "\n",
    "# idf = {}\n",
    "# num = len(crawledList)\n",
    "\n",
    "# for key in invertedIndex.keys():\n",
    "#     df = len(invertedIndex[key].keys())\n",
    "#     idf[key] = np.log2(num/df)\n",
    "    \n",
    "# tfIdf = copy.deepcopy(invertedIndex)\n",
    "\n",
    "# for token in tfIdf:\n",
    "#     for entry in tfIdf[token]:\n",
    "#         tf = tfIdf[token][entry] / mostFreq[entry]\n",
    "#         tfIdf[token][entry] = tf * idf[token]\n",
    "        \n",
    "# def documentLength(doc, tokens):\n",
    "#     documentLen = 0\n",
    "    \n",
    "#     for token in set(tokens):\n",
    "#         documentLen += tfIdf[token][doc] ** 2\n",
    "    \n",
    "#     documentLen = np.sqrt(documentLen)\n",
    "#     return documentLen\n",
    "\n",
    "# webpageLengths = {}\n",
    "\n",
    "# for page in pageTokens:\n",
    "#     webpageLengths[page] = documentLength(page, pageTokens[page])\n",
    "    \n",
    "# def queryTokenize(query):\n",
    "#     temp = query.lower()\n",
    "#     temp = temp.split()\n",
    "#     temp = [ps.stem(word) for word in temp]\n",
    "#     temp = [item for item in temp if item not in stop_list]\n",
    "#     temp = [item for item in temp if len(item) > 2]\n",
    "#     temp = (\" \").join(temp)\n",
    "#     for i in punctuation:\n",
    "#         temp = temp.replace(i, \"\")\n",
    "#     temp = re.sub(\"\\d\", \"\", temp)\n",
    "#     return temp.split()\n",
    "\n",
    "# def cosineSimilarity(documents, query):\n",
    "#     similarityScores = {}\n",
    "#     queryLen = 0\n",
    "#     queryWeights = {}\n",
    "    \n",
    "#     queryDict = Counter(query)\n",
    "    \n",
    "#     for token in queryDict.keys():\n",
    "#         tf = queryDict[token] / queryDict.most_common(1)[0][1]\n",
    "#         queryWeights[token] = tf * idf.get(token, 0)\n",
    "#         queryLen += queryWeights[token] ** 2\n",
    "    \n",
    "#     queryLen = np.sqrt(queryLen)\n",
    "    \n",
    "#     for word in query:\n",
    "#         wordWeight = queryWeights.get(word)\n",
    "        \n",
    "#         if wordWeight:\n",
    "#             for page in tfIdf[word].keys():\n",
    "#                 similarityScores[page] = similarityScores.get(page, 0) + (tfIdf[word][page] * wordWeight)\n",
    "    \n",
    "#     for page in similarityScores:\n",
    "#         similarityScores[page] = similarityScores[page] / (webpageLengths[page] * queryLen)\n",
    "        \n",
    "#     return sorted(similarityScores.items(), key= lambda x: x[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
